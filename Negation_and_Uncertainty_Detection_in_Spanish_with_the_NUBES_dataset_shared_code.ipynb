{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajtamayoh/ClinicalTextMining/blob/main/Negation_and_Uncertainty_Detection_in_Spanish_with_the_NUBES_dataset_shared_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nml9hXt1NHXr"
      },
      "source": [
        "# Clinical Text Mining in Spanish (Negation and Uncertainty Detection)\n",
        "\n",
        "Here you are the source code for the paper:\n",
        "\n",
        "### Clinical Text Mining in Spanish Enhanced by Negation Detection and Named Entity Recognition\n",
        "\n",
        "Authors:\n",
        "\n",
        "Antonio Tamayo (ajtamayo2019@ipn.cic.mx, ajtamayoh@gmail.com)\n",
        "\n",
        "Diego A. Burgos (burgosda@wfu.edu)\n",
        "\n",
        "Alexander Gelbulkh (gelbukh@gelbukh.com)\n",
        "\n",
        "For bugs or questions related to the code, do not hesitate to contact us (Antonio Tamayo: ajtamayoh@gmail.com)\n",
        "\n",
        "If you use this code please cite our work:\n",
        "\n",
        "Tamayo Herrera, A. J., Burgos, D. A., & Gelbukh, A. (2023). Clinical text mining in spanish enhanced by negationdetection and named entity recognition. Computación y Sistemas, 27(4), 1169-1181.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "\n",
        "To run this code you need to download the dataset (three files: JSON_NubEs_train.json, JSON_NubEs_dev.json, JSON_NubEs_test.json) at: [download dataset](https://github.com/ajtamayoh/Data_Mining_in_the_Medical_Field_in_Spanish/tree/main/Negation%20and%20Uncertainty%20in%20NUBES/Dataset)\n",
        "\n",
        "Then, you must create a folder called \"Datasets\" in the root of your Google Drive and load there both folders previously downloaded.\n",
        "\n",
        "Once the dataset is ready to use, you should [open this notebook in colab](https://colab.research.google.com/drive/1z4LcF2TFoS7TrFvBsJ825J-E8x6oiGEw?authuser=1#scrollTo=9qIVQ_sQJfn_) and save a copy in your drive."
      ],
      "metadata": {
        "id": "9qIVQ_sQJfn_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gGu8XvkQuHU"
      },
      "source": [
        "## About the infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YCD8Yn8QvCr"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDijHzOMQzwA"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WaBFjAEUc0l"
      },
      "source": [
        "## Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEX7xfLiUc0m"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K90ROU-SaLC5"
      },
      "source": [
        "## Hugging Face Authentication\n",
        "\n",
        "If you want to save your own model and make it available online we strongly recommend signing up at: https://huggingface.co/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgxCB7y8Uc0n"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hstit1gRUc0o"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"ajtamayoh@gmail.com\"\n",
        "!git config --global user.name \"ajtamayoh\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXbs9i1fUc0o"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6SkGUEUUc0p"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U64pk_flQ-nw"
      },
      "source": [
        "## Connecting to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgzIfAnfaDR1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ncKARiLoRQ"
      },
      "source": [
        "## Exploring & Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZaPi7EMLrz8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RIR3iRCUc0h"
      },
      "source": [
        "# Negation Scope Detection as a token classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfLiNt_9F3Vp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#Loading the dataset\n",
        "NubEs_tr = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Datasets/JSON_NubEs_train_HF.json\", field=\"data\")\n",
        "NubEs_val = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Datasets/JSON_NubEs_dev_HF.json\", field=\"data\")\n",
        "NubEs_test = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Datasets/JSON_NubEs_test_HF.json\", field=\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD06htLyGX_q"
      },
      "outputs": [],
      "source": [
        "NubEs_tr.shape\n",
        "NubEs_val.shape\n",
        "NubEs_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubAYCdhLG2b_"
      },
      "outputs": [],
      "source": [
        "NubEs_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9FTB991Sl12"
      },
      "source": [
        "## Building the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMZbF0mSaOiS"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "#For training, validation, and test partitions\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': NubEs_tr['train'],\n",
        "    'validation': NubEs_val['train'],\n",
        "    'test': NubEs_test['train']\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFQfd5u9Uc0q"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI0wWv0aUc0t"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"][0][\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-usgjZalK7E9"
      },
      "outputs": [],
      "source": [
        "raw_datasets['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLHqEaOVUc0u"
      },
      "outputs": [],
      "source": [
        "#Para NSD y Uncertainty Scope Detection (USD)\n",
        "label_names = [\n",
        "                                \"O\",        # 0\n",
        "                                \"B-UNC\",    # 1\n",
        "                                \"B-NSCO\",   # 2\n",
        "                                \"B-USCO\",   # 3\n",
        "                                \"B-NEG\",    # 4\n",
        "                                \"I-UNC\",    # 5\n",
        "                                \"I-NSCO\",   # 6\n",
        "                                \"I-USCO\",   # 7\n",
        "                                \"I-NEG\",    # 8\n",
        "                            ]\n",
        "label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iCYOiwZUc0u"
      },
      "outputs": [],
      "source": [
        "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
        "labels = [int(n) for n in raw_datasets[\"train\"][0][\"ner_tags\"]]\n",
        "#labels = raw_datasets[\"train\"][0][\"pos_tags\"]\n",
        "#labels = raw_datasets[\"train\"][0][\"chunk_tags\"]\n",
        "line1 = \"\"\n",
        "line2 = \"\"\n",
        "for word, label in zip(words, labels):\n",
        "    full_label = label_names[label]\n",
        "    max_length = max(len(word), len(full_label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
        "\n",
        "print(line1)\n",
        "print(line2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRKUqAVVbvf7"
      },
      "source": [
        "## Loading BERT-based model as a pre-trained one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuZTRxLPUc0v"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#model_checkpoint = \"PlanTL-GOB-ES/roberta-base-biomedical-clinical-es\"\n",
        "model_checkpoint = \"bert-base-multilingual-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr7qjUe7Uc0v"
      },
      "outputs": [],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbchCgPGUc0v"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "inputs.tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udiVaGenUc0w"
      },
      "outputs": [],
      "source": [
        "inputs.word_ids()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"El paciente hipertenso no presenta fiebre ni infección.\"]).tokens()"
      ],
      "metadata": {
        "id": "1JfXjIW5cKH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhu4Cl3-Uc0w"
      },
      "outputs": [],
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H820wg0sUc0x"
      },
      "outputs": [],
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-ZxnhGSUc0x"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INNjCms4Uc0y"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFpUXjvmUc0y"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHCuXD5EUc0y"
      },
      "outputs": [],
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
        "batch[\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6GW37IPUc0z"
      },
      "outputs": [],
      "source": [
        "for i in range(2):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDjn2La4Uc0z"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mqjInn4Uc0z"
      },
      "outputs": [],
      "source": [
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIaLt4n-Uc00"
      },
      "outputs": [],
      "source": [
        "predictions = labels.copy()\n",
        "predictions[2] = \"B-NSCO\"\n",
        "metric.compute(predictions=[predictions], references=[labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xewJhf6Uc00"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "\n",
        "        #per label\n",
        "\n",
        "        #Negation\n",
        "        \"NEG_precision\": all_metrics[\"NEG\"]['precision'],\n",
        "        \"NEG_recall\": all_metrics[\"NEG\"]['recall'],\n",
        "        \"NEG_F1\": all_metrics[\"NEG\"]['f1'],\n",
        "        \"NSCO_precision\": all_metrics[\"NSCO\"]['precision'],\n",
        "        \"NSCO_recall\": all_metrics[\"NSCO\"]['recall'],\n",
        "        \"NSCO_F1\": all_metrics[\"NSCO\"]['f1'],\n",
        "        #Uncertainty\n",
        "        \"UNC_precision\": all_metrics[\"UNC\"]['precision'],\n",
        "        \"UNC_recall\": all_metrics[\"UNC\"]['recall'],\n",
        "        \"UNC_F1\": all_metrics[\"UNC\"]['f1'],\n",
        "        \"USCO_precision\": all_metrics[\"USCO\"]['precision'],\n",
        "        \"USCO_recall\": all_metrics[\"USCO\"]['recall'],\n",
        "        \"USCO_F1\": all_metrics[\"USCO\"]['f1'],\n",
        "\n",
        "        #Overall\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdmwR_2HUc00"
      },
      "outputs": [],
      "source": [
        "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svP0J1bWs3pK"
      },
      "outputs": [],
      "source": [
        "id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvIvq5wxs8P8"
      },
      "outputs": [],
      "source": [
        "label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKNvor68cVQs"
      },
      "source": [
        "## Changing the head of prediction for NER under the IO scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Rrve75Uc00"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    num_labels = 9,    # For join NSD and USD\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDiAYqsYUc00"
      },
      "outputs": [],
      "source": [
        "model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS-L0U1XUc01"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    #\"Negation_and_Uncertainty_Scope_Detection_RoBERTa_fine_tuned_your_identifier\", # Training + Test dataset\n",
        "    #\"Negation_and_Uncertainty_Scope_Detection_RoBERTa_fine_tuned_dev_your_identifier\", # Training + Dev dataset\n",
        "    #\"Negation_and_Uncertainty_Scope_Detection_mBERT_fine_tuned_your_identifier\", # Training + Test dataset\n",
        "    \"Negation_and_Uncertainty_Scope_Detection_mBERT_fine_tuned_dev_your_identifier\", # Training + Dev dataset\n",
        "\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=7,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdHv4CMvctoB"
      },
      "source": [
        "## Fine-tuning mBERT for Negation Scope Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvHls_JJUc01"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    #eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rGdJ42qc3sh"
      },
      "source": [
        "## Saving the fine-tuned model at Hugging Face (It requires previous authentication)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAvTQZClUc01"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(commit_message=\"Fine-tuning completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjEM4d77Toxs"
      },
      "source": [
        "## Loading the model for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKO5fyKyUc04"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Replace this with your own checkpoint.\n",
        "\n",
        "model_checkpoint = \"user_name/model_identifier\"\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}